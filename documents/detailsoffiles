# Project Files Overview: Non-Pipeline Components

This document details the files in the project that are **not** part of the direct video processing pipeline (Video -> Index -> Search). These are utility, maintenance, and setup scripts that support the main system.

## 1. Utility & Maintenance Scripts

These scripts are used for debugging, managing the database, or one-off tasks. They do not run automatically during the standard processing flow.

*   **`mark_samples.py`**
    *   **Purpose**: Manually flags specific named videos in the `videosearch.db` database to appear in the "Sample Images" section of the web interface.
    *   **Usage**: Run `python mark_samples.py` to update the user interface with specific sample entries.
    *   **Connection**: Modifies `videosearch.db`, which `web_app.py` reads to display samples.

*   **`list_videos.py`**
    *   **Purpose**: acts as a **system dashboard** that tells you exactly what is stored in the `videosearch.db` database.
    *   **Detailed Function**: It connects to the database and prints a formatted list of every video known to the system. For each video, it displays:
        *   **Filename**: The name of the video file.
        *   **Indexed Status**: `True` if the video has been processed (frames extracted, features computed, added to index), `False` if it is still pending.
        *   **Sample Status**: `True` if it is set to appear in the "Sample Images" UI, `False` otherwise.
    *   **Why use it?**: If you add a new video and aren't sure if it was detected, or if you want to know which videos are currently visible as samples, run this script.
    *   **Connection**: Reads strictly from `videosearch.db`. Safe to run anytime.

*   **`check_index.py`**
    *   **Purpose**: A diagnostic script to verify the integrity and content of the `index.npy` file.
    *   **Usage**: Run to confirm that the index file is readable and contains the expected keys (video/frame names).
    *   **Connection**: Reads `work_dir/index.npy` directly, bypassing the database.


## 2. Setup & Migration Scripts

*   **`populate_db.py`**
    *   **Purpose**: Initially populates the database by scanning the `static/videos/` directory.
    *   **Usage**: Run once during initial setup or if the database is deleted/reset.
    *   **Connection**: Writes initial entries to `videosearch.db`.

*   **`migrate_db.py`**
    *   **Purpose**: Handles database schema updates (e.g., adding new columns like `show_in_samples` if they didn't exist in older versions).
    *   **Usage**: Run if you are upgrading the code and need to update an old database file.

*   **`download_news_videos.py`**
    *   **Purpose**: Downloads video content from YouTube (specifically Indian news channels as requested).
    *   **Usage**: Standalone script to fetch new raw data.
    *   **Connection**: Places files into `static/videos/` or `downloaded_videos/`, which acts as the *input* for the main pipeline.

## 3. Configuration & Metadata

*   **`requirements.txt`**
    *   **Purpose**: Lists all Python library dependencies (e.g., `flask`, `opencv-python`, `numpy`).
    *   **Usage**: `pip install -r requirements.txt`.

*   **`.gitignore`**
    *   **Purpose**: Tells Git which files to ignore (e.g., large video files, virtual environments, database files).

*   **`README.md`**
    *   **Purpose**: General entry point documentation for the repository.

## 4. Documentation Folder (`documents/`)

This folder contains detailed project knowledge bases.

*   **`documents/PROJECT_DETAILS.md`**: Comprehensive tech stack, rules, and setup guide.
*   **`documents/algorithmdetails.md`**: Explanations of SIFT, GMM, Fisher Vectors, and other core algorithms.
*   **`documents/Pipeline`**: Detail on the *Core Pipeline* (the files NOT listed here).
*   **`documents/check`**: Status report on project directories (.agent, .dist).
*   **`documents/files.md`**: A legacy file list (superseded by this document and `Pipeline`).

## 5. Storage & Architecture FAQ

### Q: Are videos in `static/videos` considered a database?
*   **No, this is "File System Storage".**
*   **Explanation**: The actual video files (`.mp4`) are stored as raw files in the folder `static/videos`. This is **not** a database; it is just a directory on your hard drive.
*   **Role of `videosearch.db`**: The SQLite database only stores the **Reference (Metadata)** (e.g., the filename `video1.mp4` and its path). It points to the file, but doesn't contain the file itself.
*   **Why do we do this?**: Storing large video files directly inside a database is generally bad practice. It makes the database huge, slow, and hard to back up. The standard approach is **Files on Disk + Paths in Database**.

### Q: Can we use Neon DB to store videos to save local space?
*   **Answer**: You **can** use Neon for the database part, but **NOT** for storing the actual video files.
*   **Why?**:
    1.  **Neon is for Metadata**: Neon is a serverless PostgreSQL database. It is excellent for replacing your local `videosearch.db` file (storing the list of videos, samples, etc.).
    2.  **Not for BLOBs**: Storing Gigabytes of video data directly inside a Postgres database (Neon) is extremely expensive and inefficient. It will slow down queries and likely exceed free tier limits quickly.
    *   **Step 2**: Use **Neon** (or keep SQLite) to store the links/URLs to those cloud files.

### Q: Can I store videos in a "Vector Database" (like PostgreSQL with pgvector)?
*   **Short Answer**: **NO. Vector databases are NOT for storing video files.**
*   **Detailed Explanation**:
    *   **What Vector Databases Store**: Vector databases (like PostgreSQL with pgvector extension) are designed to store **feature vectors** (embeddings) - arrays of numbers that represent the mathematical "meaning" of content.
    *   **Our Current Setup**:
        *   `index.npy` stores Fisher Vectors (feature embeddings extracted from video frames)
        *   `videosearch.db` stores metadata (filenames, paths, indexed status)
        *   `static/videos/` stores actual video files (.mp4)
    *   **Can PostgreSQL/pgvector Replace Our Index?**: **YES** - You could migrate `index.npy` to PostgreSQL with pgvector to store the Fisher Vector embeddings. This would enable:
        *   Better scalability for large datasets
        *   Built-in similarity search using pgvector's operators
        *   Centralized storage of both metadata and vectors
    *   **Should PostgreSQL Store Video Files?**: **ABSOLUTELY NOT**
        *   Video files are Binary Large Objects (BLOBs), not vectors
        *   Storing videos in Postgres (using `BYTEA` columns) will:
            *   Bloat your database to gigabytes
            *   Make queries extremely slow
            *   Complicate backups and replication
            *   Exceed free tier limits on hosted Postgres (like Neon)
*   **Recommended Architecture**:
    *   **PostgreSQL (with pgvector)**: Store metadata + Fisher Vector embeddings
    *   **Object Storage (Cloudinary/etc)**: Store actual video files
    *   **Your Current Metadata is FINE**: If you're happy with SQLite for metadata, keep it! No need to change.

### Q: What are FREE alternatives to AWS S3 for storing video files?
**Note**: AWS S3 has a free tier (5 GB for 12 months), but here are better long-term free options:

1.  **Cloudinary (BEST for this project)**
    *   **Why Best**: Purpose-built for video/image hosting with automatic optimization
    *   **Free Tier**: 25 GB storage + 25 GB bandwidth/month (permanent free tier)
    *   **Features**: 
        *   Direct video streaming URLs
        *   Automatic transcoding and compression
        *   CDN included (fast global delivery)
        *   Simple Python SDK
    *   **Setup**: Sign up → Get API credentials → Upload videos via SDK
    *   **Verdict**: ⭐ **Recommended - Most generous free tier, easiest integration**

2.  **Backblaze B2 (Best S3 alternative)**
    *   **Free Tier**: 10 GB storage + 1 GB daily download (permanent)
    *   **Pros**: 
        *   S3-compatible API (easy migration if you know S3)
        *   Cheaper than S3 if you exceed free tier
        *   No egress fees for Cloudflare CDN integration
    *   **Cons**: Requires more setup than Cloudinary
    *   **Verdict**: Good if you want S3-like experience without AWS costs

3.  **Firebase Storage (Google)**
    *   **Free Tier**: 5 GB storage + 1 GB/day download
    *   **Pros**: Easy integration, backed by Google Cloud
    *   **Cons**: Smaller free tier than Cloudinary
    *   **Verdict**: Good for small demos, but limited for video projects

4.  **Supabase Storage**
    *   **Free Tier**: 1 GB storage (permanent)
    *   **Pros**: Open-source, includes PostgreSQL database
    *   **Cons**: Very limited storage for videos
    *   **Verdict**: Not ideal for video-heavy projects

5.  **GitHub Large File Storage (LFS)** *(Not recommended)*
    *   **Free Tier**: 1 GB storage + 1 GB bandwidth/month
    *   **Cons**: Designed for version control, not video streaming
    *   **Verdict**: Avoid for this use case

### Q: Should I migrate to cloud storage now?
*   **If your project is local/demo**: Keep videos in `static/videos/` - it's simple and works
*   **If you want to deploy online**: Use **Cloudinary** - it's free, fast, and designed for videos
*   **If you want to learn cloud architecture**: Try **Backblaze B2** for S3-compatible experience

**Final Recommendation**: For this project, use **Cloudinary** (free 25GB) for video storage. Your current metadata approach with SQLite is perfectly fine - no need to change it unless you're scaling to thousands of videos.
